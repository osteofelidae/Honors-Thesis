{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Reference-Guided Stacked Hourglass Network for Facial Landmark Detection\n",
        "Albert Kim - Undergraduate Honors Thesis"
      ],
      "metadata": {
        "id": "4BLQVQnY8Lox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "4BhgwScj8WYR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5NrcU8_8H3c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "DATA_ROOT_DIR = \"/content\"\n",
        "IMG_DIR = os.path.join(DATA_ROOT_DIR, \"extracted_data\", \"WFLW_images\")\n",
        "TRAIN_LANDMARK_FILE_PATH = os.path.join(DATA_ROOT_DIR, \"extracted_annotations\", \"WFLW_annotations\", \"list_98pt_rect_attr_train_test\", \"list_98pt_rect_attr_train.txt\")\n",
        "TEST_LANDMARK_FILE_PATH = os.path.join(DATA_ROOT_DIR, \"extracted_annotations\", \"WFLW_annotations\", \"list_98pt_rect_attr_train_test\", \"list_98pt_rect_attr_test.txt\")\n",
        "IMAGE_RESIZE = (128, 128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os\n",
        "\n",
        "# WFLW Images\n",
        "file_id = '1hzBd48JIdWTJSsATBEB_eFVvPL1bx6UC'\n",
        "output_path = '/content/WFLW_images.tar.gz'\n",
        "gdown.download(id=file_id, output=output_path, quiet=False)\n",
        "\n",
        "extract_dir = '/content/extracted_data'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "!tar -xvf {output_path} -C {extract_dir}\n",
        "\n",
        "print(f\"File downloaded to {output_path}\")\n",
        "print(f\"File extracted to {extract_dir}\")\n",
        "\n",
        "# WFLW Annotations\n",
        "annotations_url = 'https://wywu.github.io/projects/LAB/support/WFLW_annotations.tar.gz'\n",
        "annotations_output_path = '/content/WFLW_annotations.tar.gz'\n",
        "annotations_extract_dir = '/content/extracted_annotations'\n",
        "os.makedirs(annotations_extract_dir, exist_ok=True)\n",
        "\n",
        "!wget {annotations_url} -O {annotations_output_path}\n",
        "!tar -xzvf {annotations_output_path} -C {annotations_extract_dir}\n",
        "\n",
        "print(f\"Annotations downloaded to {annotations_output_path}\")\n",
        "print(f\"Annotations extracted to {annotations_extract_dir}\")"
      ],
      "metadata": {
        "id": "yNyL01we8Vlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "q_9CAFUJ8pwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2\n",
        "import glob\n",
        "from typing import Union\n",
        "from tqdm import tqdm\n",
        "import json"
      ],
      "metadata": {
        "id": "ZIG7LJO18sUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class"
      ],
      "metadata": {
        "id": "XEKMVrYu8u6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_heatmap(landmark, heatmap_size, sigma=5):\n",
        "\n",
        "    x, y = landmark\n",
        "    H, W = heatmap_size\n",
        "    heatmap = np.zeros((H, W), dtype=np.float32)\n",
        "\n",
        "    # Ensure bounds\n",
        "    if x < 0 or y < 0 or x >= W or y >= H:\n",
        "        return heatmap\n",
        "    # Generate heatmap\n",
        "    xv, yv = np.meshgrid(np.arange(W), np.arange(H))\n",
        "    heatmap = np.exp(-((xv - x) ** 2 + (yv - y) ** 2) / (2 * sigma ** 2))\n",
        "    return heatmap\n",
        "\n",
        "def heatmaps_to_coordinates(heatmaps, tau=0.05):\n",
        "    B, N, H, W = heatmaps.shape\n",
        "    heatmaps_flat = heatmaps.view(B, N, -1)\n",
        "    probs = F.softmax(heatmaps_flat / tau, dim=-1)\n",
        "\n",
        "    y_coords = torch.arange(H, device=heatmaps.device, dtype=torch.float32)\n",
        "    x_coords = torch.arange(W, device=heatmaps.device, dtype=torch.float32)\n",
        "    yy, xx = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
        "    xx = xx.reshape(-1)\n",
        "    yy = yy.reshape(-1)\n",
        "\n",
        "    x = torch.sum(probs * xx, dim=-1)\n",
        "    y = torch.sum(probs * yy, dim=-1)\n",
        "    return torch.stack([x, y], dim=-1)\n",
        "\n",
        "class WFLWLandmarksDataset_ReferenceData(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, landmark_file, transform=None, heatmap_size=IMAGE_RESIZE, max_images=None, attr_map=None):\n",
        "\n",
        "        # Default transform\n",
        "        if transform is None:\n",
        "          transform = transforms.Compose([\n",
        "              transforms.Resize(IMAGE_RESIZE),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "          ])\n",
        "\n",
        "        # Set instance vars\n",
        "        self.img_dir = img_dir\n",
        "        self.landmark_file = landmark_file\n",
        "        self.transform = transform\n",
        "        self.heatmap_size = heatmap_size\n",
        "        self.max_images = max_images\n",
        "        self.attr_map = attr_map\n",
        "\n",
        "        self.data = self._load_data()\n",
        "        self.data_typed = {}\n",
        "\n",
        "    def _load_data(self):\n",
        "\n",
        "        with open(self.landmark_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        # Limit number of images (depreciated)\n",
        "        if self.max_images is not None:\n",
        "            lines = lines[:self.max_images]\n",
        "\n",
        "        data_list = []\n",
        "        for line in lines:\n",
        "            tokens = line.strip().split()\n",
        "            if len(tokens) != 207:  # 196 landmarks, 4 bounding box, 6 attrs, 1 img name\n",
        "                continue\n",
        "\n",
        "            landmarks = list(map(float, tokens[:196]))\n",
        "            rect = list(map(float, tokens[196:200]))\n",
        "            attrs = list(map(float, tokens[200:206]))\n",
        "            img_id = tokens[206]\n",
        "\n",
        "\n",
        "            # Resize bounding box\n",
        "            x1, y1, x2, y2 = rect\n",
        "            scale = 1.2\n",
        "\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            cx = x1 + w / 2\n",
        "            cy = y1 + h / 2\n",
        "\n",
        "            new_w = w * scale\n",
        "            new_h = h * scale\n",
        "\n",
        "            rect = [\n",
        "                cx - new_w / 2,\n",
        "                cy - new_h / 2,\n",
        "                cx + new_w / 2,\n",
        "                cy + new_h / 2\n",
        "            ]\n",
        "\n",
        "            if self.attr_map is None or self.attr_map == attrs:\n",
        "                data_list.append({\n",
        "                    'img_id': img_id,\n",
        "                    'landmarks': landmarks,\n",
        "                    'rect': rect,\n",
        "                    'attributes': attrs\n",
        "                })\n",
        "\n",
        "        return data_list\n",
        "\n",
        "    def _crop_image_by_rect(self, image, rect):\n",
        "      x1, y1, x2, y2 = map(int, rect)\n",
        "      h, w = image.shape[:2]\n",
        "\n",
        "      # Calculate padding\n",
        "      pad_left   = max(0, -x1)\n",
        "      pad_top    = max(0, -y1)\n",
        "      pad_right  = max(0, x2 - w)\n",
        "      pad_bottom = max(0, y2 - h)\n",
        "\n",
        "      # Apply padding if any\n",
        "      if any([pad_left, pad_top, pad_right, pad_bottom]):\n",
        "          image = np.pad(\n",
        "              image,\n",
        "              ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)),\n",
        "              mode='constant',\n",
        "              constant_values=0\n",
        "          )\n",
        "\n",
        "          # Adjust coordinates for padding\n",
        "          x1 += pad_left\n",
        "          y1 += pad_top\n",
        "          x2 += pad_left\n",
        "          y2 += pad_top\n",
        "\n",
        "      cropped = image[y1:y2, x1:x2]\n",
        "      return cropped\n",
        "\n",
        "    def _adjust_landmarks_to_crop(self, landmarks, rect):\n",
        "        x1, y1, x2, y2 = rect\n",
        "        adjusted_landmarks = landmarks.copy()\n",
        "        adjusted_landmarks[:, 0] -= x1  # Adjust x\n",
        "        adjusted_landmarks[:, 1] -= y1  # Adjust y\n",
        "        return adjusted_landmarks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # MAIN IMAGE\n",
        "        item = self.data[idx]\n",
        "        img_path = os.path.join(self.img_dir, item['img_id'])\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Crop main image by box\n",
        "        image_cropped = self._crop_image_by_rect(image, item['rect'])\n",
        "        H_img_cropped, W_img_cropped = image_cropped.shape[:2]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image_tensor = self.transform(Image.fromarray(image_cropped))\n",
        "        else:\n",
        "            image_tensor = torch.from_numpy(image_cropped.transpose(2,0,1) / 255.0).float()\n",
        "        H_img, W_img = image_tensor.shape[1:3]\n",
        "\n",
        "        # HEATMAPS\n",
        "        landmarks = np.array(item['landmarks'], dtype=np.float32).reshape(-1,2)\n",
        "\n",
        "        # Adjust landmarks to crop\n",
        "        landmarks_adjusted = self._adjust_landmarks_to_crop(landmarks, item['rect'])\n",
        "\n",
        "        # Scale lanemarks\n",
        "        scale_x_landmark = self.heatmap_size[0] / W_img_cropped  # Transform resizes to 256x256\n",
        "        scale_y_landmark = self.heatmap_size[1] / H_img_cropped\n",
        "\n",
        "        landmarks_adjusted[:, 0] *= scale_x_landmark\n",
        "        landmarks_adjusted[:, 1] *= scale_y_landmark\n",
        "\n",
        "        heatmaps = np.zeros((landmarks_adjusted.shape[0], self.heatmap_size[0], self.heatmap_size[1]), dtype=np.float32)\n",
        "        for i, (x, y) in enumerate(landmarks_adjusted):\n",
        "\n",
        "            # Skip landmarks outside crop\n",
        "            if 0 <= x < W_img and 0 <= y < H_img:\n",
        "                heatmaps[i] = generate_heatmap((x, y), self.heatmap_size)\n",
        "                #print(f\"ACCEPTED: {img_path} LANDMARK {i}\")\n",
        "            #else:\n",
        "            #    print(f\"SKIPPED {idx}: {img_path} LANDMARK {i}\")\n",
        "        heatmaps_tensor = torch.from_numpy(heatmaps)\n",
        "\n",
        "        # REFERENCE IMAGE\n",
        "        target_attrs = [0, 0, 0, 0, 0, 0] #item['attributes']\n",
        "        target_attrs_str = ','.join(map(str, target_attrs))\n",
        "\n",
        "        # Find all normal attrs\n",
        "        if self.data_typed.get(target_attrs_str) is None:\n",
        "          self.data_typed[target_attrs_str] = [\n",
        "              i for i, data_item in enumerate(self.data)\n",
        "              if data_item['attributes'] == [0, 0, 0, 0, 0, 0] and i != idx\n",
        "          ]\n",
        "        matching_indices = self.data_typed[target_attrs_str]\n",
        "\n",
        "        if matching_indices:\n",
        "            # Randomly select from images\n",
        "            ref_idx = random.choice(matching_indices)\n",
        "        else:\n",
        "            # If no matching images, use any image\n",
        "            ref_idx = idx\n",
        "            while ref_idx == idx:\n",
        "                ref_idx = random.randint(0, len(self.data)-1)\n",
        "\n",
        "        ref_item = self.data[ref_idx]\n",
        "\n",
        "        ref_img_path = os.path.join(self.img_dir, ref_item['img_id'])\n",
        "        ref_image = cv2.imread(ref_img_path)\n",
        "        if ref_image is None:\n",
        "            raise FileNotFoundError(f\"Reference image not found: {ref_img_path}\")\n",
        "        ref_image = cv2.cvtColor(ref_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Crop reference image\n",
        "        ref_image_cropped = self._crop_image_by_rect(ref_image, ref_item['rect'])\n",
        "\n",
        "        H_ref_img_cropped, W_ref_img_cropped = ref_image_cropped.shape[:2]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            ref_image_tensor = self.transform(Image.fromarray(ref_image_cropped))\n",
        "        else:\n",
        "            ref_image_tensor = torch.from_numpy(ref_image_cropped.transpose(2,0,1) / 255.0).float()\n",
        "\n",
        "        ref_landmarks = np.array(ref_item['landmarks'], dtype=np.float32).reshape(-1,2)\n",
        "\n",
        "        # REFERENCE HEATMAPS\n",
        "        ref_landmarks_adjusted = self._adjust_landmarks_to_crop(ref_landmarks, ref_item['rect'])\n",
        "\n",
        "        scale_x_landmark = self.heatmap_size[0] / W_ref_img_cropped  # Transform resizes to 256x256\n",
        "        scale_y_landmark = self.heatmap_size[1] / H_ref_img_cropped\n",
        "\n",
        "        # Scale landmakrs\n",
        "        ref_landmarks_adjusted[:, 0] *= scale_x_landmark\n",
        "        ref_landmarks_adjusted[:, 1] *= scale_y_landmark\n",
        "\n",
        "        ref_heatmaps = np.zeros((ref_landmarks_adjusted.shape[0], self.heatmap_size[0], self.heatmap_size[1]), dtype=np.float32)\n",
        "        for i, (x, y) in enumerate(ref_landmarks_adjusted):\n",
        "\n",
        "            # Skip landmarks outside box\n",
        "            if 0 <= x < W_img and 0 <= y < H_img:\n",
        "                #x_scaled = x * scale_x\n",
        "                #y_scaled = y * scale_y\n",
        "                ref_heatmaps[i] = generate_heatmap((x, y), self.heatmap_size)\n",
        "        ref_heatmaps_tensor = torch.from_numpy(ref_heatmaps)\n",
        "\n",
        "        return image_tensor, heatmaps_tensor, ref_image_tensor, ref_heatmaps_tensor, \\\n",
        "              item['img_id'], landmarks_adjusted, ref_item['img_id'], ref_landmarks_adjusted"
      ],
      "metadata": {
        "id": "MaRzbb1r8uU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Class"
      ],
      "metadata": {
        "id": "kCdkFwnp815x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class HourglassModule(nn.Module):\n",
        "    def __init__(self, n_channels, depth):\n",
        "        super(HourglassModule, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.n_channels = n_channels\n",
        "\n",
        "        self.up1 = ConvBlock(n_channels, n_channels)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.low1 = ConvBlock(n_channels, n_channels)\n",
        "\n",
        "        if depth > 1:\n",
        "            self.low2 = HourglassModule(n_channels, depth - 1)\n",
        "        else:\n",
        "            self.low2 = ConvBlock(n_channels, n_channels)\n",
        "\n",
        "        self.low3 = ConvBlock(n_channels, n_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        up1 = self.up1(x)\n",
        "        down = self.pool(x)\n",
        "        down = self.low1(down)\n",
        "        down = self.low2(down)\n",
        "        down = self.low3(down)\n",
        "\n",
        "        up2 = F.interpolate(down, size=up1.shape[2:], mode='nearest')  # <— FIX HERE\n",
        "        return up1 + up2\n",
        "\n",
        "def get_preds_from_heatmaps(heatmaps):\n",
        "    B, N, H, W = heatmaps.shape\n",
        "    heatmaps_reshaped = heatmaps.view(B, N, -1)\n",
        "    idx = torch.argmax(heatmaps_reshaped, dim=2)\n",
        "    preds_y = (idx // W).float()\n",
        "    preds_x = (idx % W).float()\n",
        "    preds = torch.stack((preds_x, preds_y), dim=2)\n",
        "    return preds"
      ],
      "metadata": {
        "id": "W7XxbCCb82_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "class WingLoss(nn.Module):\n",
        "    def __init__(self, w = 10.0, epsilon = 2.0):\n",
        "        super().__init__()\n",
        "        self.w = w\n",
        "        self.epsilon = epsilon\n",
        "        self.c = w * (1.0 - math.log(1.0 + w / epsilon))\n",
        "\n",
        "    def forward(self, pred_coords, true_coords):\n",
        "\n",
        "        # Ensure float tensors, for mixed-precision\n",
        "        pred_coords = pred_coords.float()\n",
        "        true_coords = true_coords.float()\n",
        "\n",
        "        # Calc distance\n",
        "        diff = pred_coords - true_coords\n",
        "        abs_diff = torch.abs(diff)\n",
        "\n",
        "        # Wing loss per landmark\n",
        "        loss_small = self.w * torch.log(1.0 + abs_diff / self.epsilon)\n",
        "        loss_large = abs_diff - self.c\n",
        "\n",
        "        # Boundary\n",
        "        loss = torch.where(abs_diff < self.w, loss_small, loss_large)\n",
        "\n",
        "        # Mean loss\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "QK_xrl-p84Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StackedHourglassNet_ReferenceData(nn.Module):\n",
        "\n",
        "    def __init__(self, num_landmarks=98, num_channels=128, depth=4, num_hourglass_modules=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Images stem\n",
        "        self.shared_stem = nn.Sequential(\n",
        "            ConvBlock(3, 32),\n",
        "            ConvBlock(32, 64),\n",
        "            ConvBlock(64, num_channels)\n",
        "        )\n",
        "\n",
        "        # Hourglass backbone\n",
        "        self.shared_hourglass = nn.ModuleList([\n",
        "            HourglassModule(num_channels, depth) for _ in range(num_hourglass_modules)\n",
        "        ])\n",
        "        self.shared_intermediate_convs = nn.ModuleList([\n",
        "            ConvBlock(num_channels, num_channels) for _ in range(num_hourglass_modules)\n",
        "        ])\n",
        "\n",
        "        # Ref heatmap stem\n",
        "        self.stem_ref_heatmaps = nn.Sequential(\n",
        "            ConvBlock(num_landmarks, 128),\n",
        "            ConvBlock(128, num_channels)\n",
        "        )\n",
        "\n",
        "        # Flow predictor\n",
        "        self.flow_predictor = nn.Sequential(\n",
        "            HourglassModule(num_channels*2, depth),\n",
        "            ConvBlock(num_channels*2, 2, kernel_size=1, stride=1, padding=0),\n",
        "        )\n",
        "        # Final heatmap predictor\n",
        "        self.heatmap_predictor = nn.Sequential(\n",
        "            HourglassModule(num_channels*2, depth),\n",
        "            ConvBlock(num_channels*2, num_landmarks, kernel_size=1, stride=1, padding=0),\n",
        "        )\n",
        "\n",
        "    def forward_through_hourglass(self, x):\n",
        "\n",
        "        features = self.shared_stem(x)\n",
        "        for i in range(len(self.shared_hourglass)):\n",
        "            features = self.shared_hourglass[i](features)\n",
        "            features = self.shared_intermediate_convs[i](features)\n",
        "        return features\n",
        "\n",
        "    def warp_heatmaps(self, heatmaps, flow):\n",
        "\n",
        "        B, C, H, W = heatmaps.shape\n",
        "\n",
        "        # Normalized meshgrid\n",
        "        grid_y, grid_x = torch.meshgrid(\n",
        "            torch.linspace(-1, 1, H, device=heatmaps.device),\n",
        "            torch.linspace(-1, 1, W, device=heatmaps.device)\n",
        "        )\n",
        "        grid = torch.stack((grid_x, grid_y), dim=-1)\n",
        "        grid = grid.unsqueeze(0).repeat(B, 1, 1, 1)\n",
        "\n",
        "        # Normalize flow to [-1,1], percent of image dimensions\n",
        "        flow_norm = torch.zeros_like(flow)\n",
        "        flow_norm[:, 0, :, :] = flow[:, 0, :, :] / ((W - 1) / 2)\n",
        "        flow_norm[:, 1, :, :] = flow[:, 1, :, :] / ((H - 1) / 2)\n",
        "        flow_norm = flow_norm.permute(0, 2, 3, 1)\n",
        "        warped_grid = grid + flow_norm\n",
        "        warped_heatmaps = F.grid_sample(heatmaps, warped_grid, align_corners=True)\n",
        "        return warped_heatmaps\n",
        "\n",
        "    def forward(self, x, ref_x, ref_heatmaps):\n",
        "\n",
        "        # Images through stems, backbone\n",
        "        x_features = self.forward_through_hourglass(x)\n",
        "        ref_img_features = self.forward_through_hourglass(ref_x)\n",
        "\n",
        "        # Ref heatmaps through sttem\n",
        "        ref_heat_features = self.stem_ref_heatmaps(ref_heatmaps)\n",
        "\n",
        "        # Get flow field\n",
        "        flow_input = torch.cat((x_features, ref_img_features), dim=1)\n",
        "        flow_field = self.flow_predictor(flow_input)\n",
        "\n",
        "        # Warp reference heatmnaps\n",
        "        warped_ref_heatmaps = self.warp_heatmaps(ref_heat_features, flow_field)\n",
        "\n",
        "        # Feature fusion\n",
        "        combined = torch.cat((x_features, warped_ref_heatmaps), dim=1)\n",
        "\n",
        "        # Final outputs\n",
        "        out_heatmaps = self.heatmap_predictor(combined)\n",
        "\n",
        "        return out_heatmaps"
      ],
      "metadata": {
        "id": "y8JpLkQI86JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Y_I4uPdF9Drq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_stacked_hourglass_refdata_wflw(\n",
        "    img_dir,\n",
        "    landmark_file,\n",
        "    model_save_path,\n",
        "    num_hourglass_modules=2,\n",
        "    train_split_ratio=0.85,\n",
        "    batch_size=32,\n",
        "    learning_rate=1e-3,\n",
        "    num_epochs=20,\n",
        "    transform=None,\n",
        "    criterion=None,\n",
        "    optimizer_class=torch.optim.Adam,\n",
        "    optimizer_params=None,\n",
        "    num_channels=128,\n",
        "    mixed_precision=True,\n",
        "    gradient_accumulation_steps=1,\n",
        "    checkpoint_dir=\"./checkpoints\",\n",
        "    resume_checkpoint=None\n",
        "):\n",
        "\n",
        "    # Create checkpoint dir\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Default transform\n",
        "    if transform is None:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(IMAGE_RESIZE),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = WFLWLandmarksDataset_ReferenceData(\n",
        "        img_dir=img_dir,\n",
        "        landmark_file=landmark_file,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        print(\"Error: Dataset is empty.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Train/validation split\n",
        "    train_size = int(train_split_ratio * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Data loader\n",
        "    num_workers = min(8, os.cpu_count() // 2)\n",
        "    pin_memory = True\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        persistent_workers=num_workers > 0,\n",
        "        prefetch_factor=2 if num_workers > 0 else None\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        persistent_workers=num_workers > 0,\n",
        "        prefetch_factor=2 if num_workers > 0 else None\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = torch.cuda.amp.GradScaler() if mixed_precision and device.type == 'cuda' else None\n",
        "\n",
        "    # Initialize model\n",
        "    model = StackedHourglassNet_ReferenceData(\n",
        "        num_hourglass_modules=num_hourglass_modules,\n",
        "        num_landmarks=98,\n",
        "        num_channels=num_channels\n",
        "    ).to(device)\n",
        "\n",
        "    # Loss\n",
        "    if criterion is None:\n",
        "        criterion = WingLoss()\n",
        "        #criterion = nn.MSELoss()\n",
        "\n",
        "    # Optimizer\n",
        "    if optimizer_params is None:\n",
        "        optimizer_params = {\"lr\": learning_rate}\n",
        "    optimizer = optimizer_class(model.parameters(), **optimizer_params)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    # Training state vars\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_model = None\n",
        "\n",
        "    # Resume from checkpoint if provided\n",
        "    if resume_checkpoint and os.path.exists(resume_checkpoint):\n",
        "        print(f\"Loading checkpoint: {resume_checkpoint}\")\n",
        "        checkpoint = torch.load(resume_checkpoint, map_location=device)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_val_loss = checkpoint['best_val_loss']\n",
        "        train_losses = checkpoint['train_losses']\n",
        "        val_losses = checkpoint['val_losses']\n",
        "\n",
        "        if scaler and 'scaler_state_dict' in checkpoint:\n",
        "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "\n",
        "        print(f\"Resumed from epoch {start_epoch}, best val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    print(\"\\nStarting Training (Reference Data)...\")\n",
        "\n",
        "    # Validation\n",
        "    @torch.no_grad()\n",
        "    def validate_epoch(model, val_loader, criterion, device):\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch in val_loader:\n",
        "            images, heatmaps, ref_images, ref_heatmaps, _, _, _, _ = batch\n",
        "            images, heatmaps = images.to(device, non_blocking=True), heatmaps.to(device, non_blocking=True)\n",
        "            ref_images, ref_heatmaps = ref_images.to(device, non_blocking=True), ref_heatmaps.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(images, ref_images, ref_heatmaps)\n",
        "            #outputs = F.interpolate(outputs, size=heatmaps.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            pred_coords = heatmaps_to_coordinates(outputs)\n",
        "            gt_coords = heatmaps_to_coordinates(heatmaps)\n",
        "\n",
        "            loss = criterion(pred_coords, gt_coords)\n",
        "            val_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        return val_loss / num_batches\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Progress bar\n",
        "        pbar = tqdm(\n",
        "            train_loader,\n",
        "            desc=f'Epoch {epoch+1}/{num_epochs}',\n",
        "            leave=True,\n",
        "            ncols=100\n",
        "        )\n",
        "\n",
        "        for i, batch in enumerate(pbar):\n",
        "            images, heatmaps, ref_images, ref_heatmaps, _, _, _, _ = batch\n",
        "\n",
        "            # Mixed precision, gradient accumulation\n",
        "            if i % gradient_accumulation_steps == 0:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Faster, probably\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            heatmaps = heatmaps.to(device, non_blocking=True)\n",
        "            ref_images = ref_images.to(device, non_blocking=True)\n",
        "            ref_heatmaps = ref_heatmaps.to(device, non_blocking=True)\n",
        "\n",
        "            # Mixed precision forward\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(images, ref_images, ref_heatmaps)\n",
        "                    #outputs = F.interpolate(outputs, size=heatmaps.shape[2:], mode='bilinear', align_corners=False)\n",
        "                    pred_coords = heatmaps_to_coordinates(outputs)\n",
        "                    gt_coords = heatmaps_to_coordinates(heatmaps)\n",
        "\n",
        "                    loss = criterion(pred_coords, gt_coords) / gradient_accumulation_steps\n",
        "                scaler.scale(loss).backward()\n",
        "            else:\n",
        "                outputs = model(images, ref_images, ref_heatmaps)\n",
        "                #outputs = F.interpolate(outputs, size=heatmaps.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "                pred_coords = heatmaps_to_coordinates(outputs)\n",
        "                gt_coords = heatmaps_to_coordinates(heatmaps)\n",
        "\n",
        "                loss = criterion(pred_coords, gt_coords) / gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "            current_loss = loss.item() * gradient_accumulation_steps\n",
        "            train_loss += current_loss\n",
        "            num_batches += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'batch_loss': f'{current_loss:.4f}',\n",
        "                'avg_loss': f'{(train_loss / num_batches):.4f}'\n",
        "            })\n",
        "\n",
        "            # Gradient accumulation and optimizer\n",
        "            if (i + 1) % gradient_accumulation_steps == 0:\n",
        "                if scaler:\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    optimizer.step()\n",
        "\n",
        "        avg_train_loss = train_loss / num_batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        avg_val_loss = validate_epoch(model, val_loader, criterion, device)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_val_loss': best_val_loss,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'config': {\n",
        "                'num_hourglass_modules': num_hourglass_modules,\n",
        "                'num_channels': num_channels,\n",
        "                'learning_rate': learning_rate,\n",
        "                'batch_size': batch_size\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if scaler:\n",
        "            checkpoint['scaler_state_dict'] = scaler.state_dict()\n",
        "\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "        # Save best model to drive\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving model...\")\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model = model.state_dict().copy()\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "            # Also save best checkpoint\n",
        "            best_checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
        "            torch.save(checkpoint, best_checkpoint_path)\n",
        "\n",
        "    print(\"\\nTraining Finished!\")\n",
        "\n",
        "    # Load best model for return\n",
        "    if best_model is not None:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    # Save final training stats\n",
        "    training_stats = {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'final_epoch': num_epochs\n",
        "    }\n",
        "\n",
        "    stats_path = os.path.join(checkpoint_dir, 'training_stats.json')\n",
        "    with open(stats_path, 'w') as f:\n",
        "        json.dump(training_stats, f, indent=2)\n",
        "\n",
        "    return model, best_val_loss, train_losses, val_losses"
      ],
      "metadata": {
        "id": "y52DFDXs9FlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "94NmJNVmAtSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = None\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model, best_loss, refdata_train_losses, refdata_val_losses = train_stacked_hourglass_refdata_wflw(\n",
        "    img_dir=IMG_DIR,\n",
        "    landmark_file=TRAIN_LANDMARK_FILE_PATH,\n",
        "    model_save_path=\"/content/drive/MyDrive/refdata.pth\",\n",
        "    train_split_ratio=0.8,\n",
        "    batch_size=32,\n",
        "    learning_rate=1e-4,\n",
        "    num_epochs=50,\n",
        "    num_hourglass_modules=2,\n",
        "    num_channels=128,\n",
        "    mixed_precision = True\n",
        ")"
      ],
      "metadata": {
        "id": "9TZYFeAv9IVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Ah9upkiy9NTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_nme(pred_landmarks, gt_landmarks, return_all=False):\n",
        "    batch_nme = []\n",
        "\n",
        "    for pred_lms, gt_lms in zip(pred_landmarks, gt_landmarks):\n",
        "\n",
        "        # Inter-ocular normalization\n",
        "        gt_left_eye = gt_lms[60]\n",
        "        gt_right_eye = gt_lms[72]\n",
        "\n",
        "        d_norm = sqrt(\n",
        "            (gt_left_eye[0] - gt_right_eye[0]) ** 2 +\n",
        "            (gt_left_eye[1] - gt_right_eye[1]) ** 2\n",
        "        )\n",
        "\n",
        "        if d_norm == 0:\n",
        "            continue  # skip invalid ones\n",
        "\n",
        "        total_error = 0.0\n",
        "        for pred_lm, gt_lm in zip(pred_lms, gt_lms):\n",
        "            total_error += sqrt(\n",
        "                (pred_lm[0] - gt_lm[0]) ** 2 +\n",
        "                (pred_lm[1] - gt_lm[1]) ** 2\n",
        "            )\n",
        "\n",
        "        nme_image = (total_error / len(gt_lms)) / d_norm\n",
        "        batch_nme.append(nme_image)\n",
        "\n",
        "    if not batch_nme:\n",
        "        return [] if return_all else 0.0\n",
        "\n",
        "    return batch_nme if return_all else np.mean(batch_nme)\n",
        "\n",
        "def calculate_auc(errors, failure_threshold=0.1, step=0.0001):\n",
        "    errors = np.array(errors)\n",
        "    errors = np.clip(errors, 0, 1.0)  # ensure valid range\n",
        "\n",
        "    x = np.arange(0, failure_threshold + step, step)\n",
        "    y = np.array([np.mean(errors <= xx) for xx in x])\n",
        "\n",
        "    auc = np.trapezoid(y, x) / failure_threshold  # normalize\n",
        "    failure_rate = np.mean(errors > failure_threshold)\n",
        "\n",
        "    return auc, failure_rate\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_nme_auc(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluation loop — computes NME and AUC (inter-ocular).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_nmes = []\n",
        "    total_nme = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, gt_heatmaps, ref_images, ref_gt_heatmaps,\n",
        "                        img_ids, gt_landmarks, ref_img_ids, ref_gt_landmarks) in enumerate(data_loader):\n",
        "\n",
        "            images, gt_heatmaps = images.to(device), gt_heatmaps.to(device)\n",
        "            ref_images, ref_gt_heatmaps = ref_images.to(device), ref_gt_heatmaps.to(device)\n",
        "\n",
        "            pred_heatmaps = model(images, ref_images, ref_gt_heatmaps)\n",
        "            if isinstance(pred_heatmaps, tuple):\n",
        "                pred_heatmaps = pred_heatmaps[0]\n",
        "\n",
        "            pred_landmarks = heatmaps_to_coordinates(pred_heatmaps.cpu())\n",
        "\n",
        "            gt_landmarks_list = [lm.tolist() for lm in gt_landmarks.numpy()] \\\n",
        "                if isinstance(gt_landmarks, torch.Tensor) else gt_landmarks\n",
        "\n",
        "            batch_nmes = calculate_nme(pred_landmarks, gt_landmarks_list, return_all=True)\n",
        "\n",
        "            batch_avg_nme = np.mean(batch_nmes)\n",
        "            batch_size = len(batch_nmes)\n",
        "\n",
        "            all_nmes.extend(batch_nmes)\n",
        "            total_nme += batch_avg_nme * batch_size\n",
        "            num_samples += batch_size\n",
        "\n",
        "            print(f\"Batch {batch_idx+1}/{len(data_loader)} | Batch NME: {batch_avg_nme:.6f}\")\n",
        "\n",
        "    final_nme = total_nme / num_samples if num_samples > 0 else 0.0\n",
        "    auc, failure_rate = calculate_auc(all_nmes, failure_threshold=0.1)\n",
        "\n",
        "    print(f\"\\n--- Evaluation Complete ---\")\n",
        "    print(f\"Samples: {num_samples}\")\n",
        "    print(f\"Mean NME: {final_nme:.6f}\")\n",
        "    print(f\"AUC (10%): {auc:.6f}\")\n",
        "    print(f\"Failure Rate (>{0.1*100:.1f}%): {failure_rate*100:.2f}%\")\n",
        "\n",
        "    return final_nme, auc, failure_rate"
      ],
      "metadata": {
        "id": "VX66q6gt9OdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize model\n",
        "model = StackedHourglassNet_ReferenceData(\n",
        "    num_hourglass_modules=2,\n",
        "    num_landmarks=98,\n",
        "    num_channels=128\n",
        ")\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/refdata_2.pth\"))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "dataset = WFLWLandmarksDataset_ReferenceData(\n",
        "    img_dir=IMG_DIR,\n",
        "    landmark_file=TEST_LANDMARK_FILE_PATH,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize(IMAGE_RESIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    #attr_map = [0, 0, 0, 0, 0, 1]\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "a = evaluate_nme_auc(model, test_loader, device)"
      ],
      "metadata": {
        "id": "Ab2z44Pc9PuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize"
      ],
      "metadata": {
        "id": "OjMpDaPZ9Slc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_prediction(model, data_loader, device, sample_idx=0):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Get batch\n",
        "    batch = next(iter(data_loader))\n",
        "    (images, gt_heatmaps, ref_images, ref_gt_heatmaps,\n",
        "     img_ids, gt_landmarks, ref_img_ids, ref_gt_landmarks) = batch\n",
        "\n",
        "    # Select sample\n",
        "    if sample_idx >= len(images):\n",
        "        sample_idx = 0\n",
        "        print(f\"Sample index out of range, using first sample instead\")\n",
        "\n",
        "    # Move to cuda\n",
        "    images = images.to(device)\n",
        "    ref_images = ref_images.to(device)\n",
        "    ref_gt_heatmaps = ref_gt_heatmaps.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        outputs = model(images, ref_images, ref_gt_heatmaps)\n",
        "        outputs = F.interpolate(outputs, size=gt_heatmaps.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Convert to pixel coords\n",
        "        lm_pred = heatmaps_to_coordinates(outputs.cpu())\n",
        "\n",
        "    # Select sample\n",
        "    image = images[sample_idx].cpu().permute(1, 2, 0).numpy()\n",
        "    pred_landmarks = lm_pred[sample_idx]\n",
        "    gt_landmarks_sample = gt_landmarks[sample_idx]\n",
        "\n",
        "    ref_image = ref_images[sample_idx].cpu().permute(1, 2, 0).numpy()\n",
        "    ref_landmarks_sample = ref_gt_landmarks[sample_idx]\n",
        "\n",
        "    # Denormalize images if in [-1, 1]\n",
        "    def denormalize(img):\n",
        "        if img.min() < 0:\n",
        "            img = (img + 1) / 2\n",
        "        return np.clip(img, 0, 1)\n",
        "\n",
        "    image = denormalize(image)\n",
        "    ref_image = denormalize(ref_image)\n",
        "\n",
        "    # Calculate NME\n",
        "    sample_nme = calculate_nme([pred_landmarks], [gt_landmarks_sample])\n",
        "\n",
        "    # Create plot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Main image: GT + preds\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].scatter([p[0] for p in gt_landmarks_sample], [p[1] for p in gt_landmarks_sample],\n",
        "                    c='green', s=10, label='GT', alpha=0.7)\n",
        "    axes[0].scatter([p[0] for p in pred_landmarks], [p[1] for p in pred_landmarks],\n",
        "                    c='red', s=10, label='Preds', alpha=0.7)\n",
        "    axes[0].set_title(f'Main Image (NME: {sample_nme:.4f})')\n",
        "    axes[0].axis('off')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Reference image: ref landmarks\n",
        "    axes[1].imshow(ref_image)\n",
        "    axes[1].scatter([p[0] for p in ref_landmarks_sample], [p[1] for p in ref_landmarks_sample],\n",
        "                    c='blue', s=10, label='Reference GT', alpha=0.7)\n",
        "    axes[1].set_title('Reference Image')\n",
        "    axes[1].axis('off')\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print info\n",
        "    print(f\"Sample ID: {img_ids[sample_idx]}\")\n",
        "    print(f\"Reference Image ID: {ref_img_ids[sample_idx]}\")\n",
        "    print(f\"Sample NME: {sample_nme:.6f}\")\n",
        "    print(f\"Image shape: {image.shape}\")\n",
        "    print(f\"Number of landmarks: {len(pred_landmarks)}\")\n",
        "\n",
        "    return {\n",
        "        'image': image,\n",
        "        'pred_landmarks': pred_landmarks,\n",
        "        'gt_landmarks': gt_landmarks_sample,\n",
        "        'nme': sample_nme,\n",
        "        'image_id': img_ids[sample_idx],\n",
        "        'ref_image': ref_image,\n",
        "        'ref_landmarks': ref_landmarks_sample,\n",
        "        'ref_image_id': ref_img_ids[sample_idx]\n",
        "    }"
      ],
      "metadata": {
        "id": "tuAf-ypR9Tr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = StackedHourglassNet_ReferenceData(\n",
        "    num_hourglass_modules=2,\n",
        "    num_landmarks=98,\n",
        "    num_channels=128\n",
        ")\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/refdata_2.pth\"))\n",
        "#model.load_state_dict(torch.load(\"/content/checkpoints/checkpoint_epoch_10.pth\")[\"model_state_dict\"])\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "dataset = WFLWLandmarksDataset_ReferenceData(\n",
        "    img_dir=IMG_DIR,\n",
        "    landmark_file=TEST_LANDMARK_FILE_PATH,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize(IMAGE_RESIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "\n",
        "    ]),\n",
        "    attr_map = [0, 0, 1, 0, 0, 0]\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "x = visualize_prediction(model, test_loader, device, sample_idx = 5)"
      ],
      "metadata": {
        "id": "Q-Lhr-269W3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}